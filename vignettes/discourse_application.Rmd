---
title: "Application in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{discourse_application}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo       = TRUE,
  message    = FALSE,
  warning    = FALSE,
  collapse   = TRUE,
  comment = "#>",
  fig.width  = 6,   
  fig.height = 4,   
  fig.align  = "left",
  dpi = 100
  )
```
Install `discourse` from Github.
```{r install, echo=TRUE, eval=FALSE}
# install devtools if needed
if (!requireNamespace("devtools")) {install.packages("devtools")}

# install from GitHub
devtools::install_github("sebastian-lortz/discourse")
```

Load the `discourse` package.
```{r load}
library(discourse)
```

## ANOVA Module
As part of the Open Science Collaboration’s large‐scale effort to estimate the replicability of psychological findings (Open-Science-Collaboration, 2015), many original datasets remain unavailable. In the study by Reynolds and Besner (2008) on contextual effects in reading aloud, participants’ response times to exception words and nonwords were measured under predictable switch and stay sequences to probe dynamic pathway control in skilled reading [https://osf.io/hasfu/]. In the following steps, I applied DISCOURSE to demonstrate how it’s ANOVA module can generate a fully synthetic dataset matching the reported summary estimates. 

I began by extracting the relevant parameters from the article.

```{r reported aov}
N = 16
levels = c(2,2)
target_group_means <- c(543, 536, 614, 618)
factor_type <- c("within", "within")
formula <- "outcome ~ Factor1 * Factor2 + Error(ID / (Factor1 * Factor2))"
integer <- FALSE
```


Factor2 and Interaction effects are reported as F < 1, thus, I set arbitrary values
```{r f values}
target_f_vec <- list(effect = c("Factor1", "Factor2", "Factor1:Factor2"),
                          F = c(30.5, 0.0, 0.2))
```

I then computed a plausible response time range [L,U] from the pooled `MSE = 3070` using
```{r range}
L <- min(target_group_means) - floor(3 * sqrt(3070))
U <- max(target_group_means) + ceiling(3 * sqrt(3070))
L
U
range <- c(370,785)
```

Next, I ran the ANOVA module with a small `max_step` to avoid early convergence given the coarse target precision and otherwise default hyperparameters.
```{r result.aov}
result.aov <- optim_aov(N = N,
                        levels = levels,
                        target_group_means = target_group_means,
                        target_f_list = target_f_vec,
                        factor_type = factor_type,
                        range = range,
                        formula = formula,
                        integer = integer,
                        max_step = .1,
                        tolerance = 1e-8,
                        max_iter = 1e3,
                        init_temp = 1,
                        cooling_rate = NULL)
```

The optimization converged exactly. Inspecting
```{r summary aov}
summary(result.aov)
```

confirms that the simulated data reproduce the published cell means and F-statistics. I then visualized the error trajectory
```{r plot error aov, fig.alt="Plot Error Trajectory for AOV."}
plot_error(result.aov)
```

and illustrated the estimated versus target effects with
```{r plot summary aov, fig.alt="Plot Summary for AOV."}
plot_summary(result.aov, standardised = FALSE)
```
. Finally, I saved the RMSE and relevant statistics,
```{r get rmse aov}
get_rmse(result.aov)
get_stats(result.aov)
```

extracted the simulated dataset and inspected its distribution.
```{r data aov, fig.alt="Plot Histogram for AOV."}
data.aov <- result.aov$data
head(data.aov)
plot_histogram(data.aov[,4, drop = FALSE])
```

Additionally, I executed the ANOVA module in multiple parallel runs to quantify both the convergence variability of RMSE within each run (compared to the target values) and the variability of RMSE across runs (compared to the average simulated values).
```{r parallel aov,results='hide', message=FALSE}
result.parallel.aov <- parallel_aov(
                        parallel_start = 100,
                        return_best_solution = FALSE,
                        N = N,
                        levels =levels,
                        target_group_means = target_group_means,
                        target_f_list = target_f_vec,
                        factor_type = factor_type,
                        range = range,
                        formula = formula,
                        tolerance = 1e-8,
                        max_iter = 1e3,
                        init_temp = 1,
                        cooling_rate = NULL,
                        integer = FALSE,
                        checkGrim = FALSE,
                        max_step = .1,
                        min_decimals = 1)
```

I then plotted these RMSE distributions side-by-side to compare within versus between run variation.
```{r plot rmse aov, fig.alt="Plot RMSE for AOV."}
plot_rmse(result.parallel.aov)
```
Note. The two clusters emerge due to the low decimal precision of the reported F values.
#

## Descriptives and LM Module

In the replication attempt by Bardwell et al. (2007) patients with obstructive sleep apnea completed both fatigue and depression scales to examine whether mood symptoms or apnea severity better predict daytime fatigue [https://doi.org/10.1016/j.jad.2006.06.013]. The original study’s (Bardwell et al., 2003) raw data are not publicly available. Here, I apply the Descriptives and LM module of the DISCOURSE framework to simulate a synthetic dataset that reproduces their reported summary estimates.

#### Step 1.
I began by extracting the relevant descriptive parameters from the article.
```{r reported vec}
N = 60
target_mean <- c(48.8, 17.3, 12.6, 10.8)
names(target_mean) <- c("Apnea.1", "Apnea.2", "Depression", "Fatigue")
target_sd <- c(27.1, 20.1, 11.3, 7.3)
integer = c(FALSE, FALSE, TRUE, TRUE)
range_matrix <- matrix(c(15, 0, 0, 0, 
                         111, 80.9, 49, 28),
                       nrow = 2, byrow = TRUE)
```
I subsequently estimated the weights 
```{r weight vec}
weight.vec <- weights_vec(
  N = N, 
  target_mean =  target_mean,
  target_sd =  target_sd, 
  range = range_matrix,
  integer = integer
)
weight.vec
```
and I ran the Descriptives module with default hyperparameters.
```{r result vec}
result.vec <- optim_vec(
  N = N,
  target_mean = target_mean,
  target_sd = target_sd,
  range = range_matrix,
  integer = integer,
  obj_weight = weight.vec,
  tolerance = 1e-8,
  max_iter = 1e5,
  max_starts = 3,
  init_temp = 1,
  cooling_rate = NULL
)
```

The optimization converged exactly. Inspecting
```{r summary vec}
summary(result.vec)
```

confirms that the simulated data reproduce the published means and SDs. I then visualized the error trajectories. For example, the error trajectory of the fatigue variable is:
```{r plot error vec, fig.alt="Plot Error for Vec."}
plot_error(result.vec, run = 4)
```

The estimated versus target descriptives are given by
```{r plot summary vec, fig.alt="Plot Summary for Vec."}
plot_summary(result.vec, standardised = FALSE)
```

Finally, I saved the RMSE and relevant statistics,
```{r get rmse vec}
get_rmse(result.vec)
get_stats(result.vec)
```

extracted the simulated dataset and inspected its distributions and frequencies.
```{r data vec, fig.width=8, fig.height=6, fig.alt="Plot Histogram for Vec."}
data.vec <- result.vec$data
head(data.vec)
gridExtra::grid.arrange(grobs = plot_histogram(data.vec), ncol = 2)
```

#### Step 2.
I began by extracting the relevant correlation and regression parameters from the article and handing off the simulated data from the Descriptives module for further use.
```{r reported lm}
sim_data <- data.vec
target_reg <- c(4.020, 0.023, 0.008, 0.438)
names(target_reg) <- c("Apnea.1", "Apnea.2", "Depression", "Fatigue")
target_se <- c(NA, 0.034, 0.048, 0.066)
target_cor <-  c(NA, NA, NA, 0.11, 0.20, 0.68)
reg_equation <- "Fatigue ~ Apnea.1 + Apnea.2 + Depression"
```

I subsequently estimated the weights 
```{r weight lm}
result.weight.lm <- weights_est(
  module = "lm",
  sim_runs = 1,
  sim_data = sim_data,
  reg_equation = reg_equation,
  target_cor = target_cor,
  target_reg = target_reg,
  target_se = target_se,
  tol = 1e-8,
  max_iter = 1e5,
  init_temp = 1,
  cooling_rate = NULL
)
weight.lm <- result.weight.lm$weights
weight.lm
```

and I ran the LM module with default hyperparameters.
```{r result lm}
result.lm <- optim_lm(
  sim_data = sim_data,
  reg_equation = reg_equation,
  target_cor = target_cor,
  target_reg = target_reg,
  target_se = target_se,
  weight = weight.lm,
  tol = 1e-8,
  max_iter = 1e5,
  max_starts = 1,
  init_temp = 1,
  cooling_rate = NULL
)
```

The optimization converged with reaching Max Iterations and a low RMSE. Inspecting
```{r summary lm}
summary(result.lm)
```

confirms that the simulated data closely reproduce the published regression and correlation parameters. I then visualized the error,
```{r plot error lm, fig.alt="Plot Error for LM."}
plot_error(result.lm)
```
the error-ratio trajectories,
```{r error ratio lm, fig.alt="Plot Error Ration for LM."}
plot_error_ratio(result.lm)
```
and illustrated the estimated versus target descriptives with
```{r plot summary lm, fig.alt="Plot Summary for LM."}
plot_summary(result.lm, standardised = FALSE)
```

Finally, I saved the RMSE and relevant statistics,
```{r get rmse lm}
get_rmse(result.lm)
get_stats(result.lm)
```


extracted the simulated dataset and inspected its partial regression plots.
```{r data lm, fig.width=11, fig.height=8, fig.alt="Plot Partial Regressions for LM."}
data.lm <- result.lm$data
head(data.lm)
partial.plots <- plot_partial_regression(lm(reg_equation, data.lm))
gridExtra::grid.arrange(grobs = partial.plots, ncol = 2)
```

Additionally, I executed the LM module in multiple parallel runs to quantify both the convergence variability of RMSE within each run (compared to the target values) and the variability of RMSE across runs (compared to the average simulated values).
```{r parallel lm,results='hide', message=FALSE}
result.parallel.lm <- parallel_lm(
  parallel_start = 100,
  return_best_solution = FALSE,
  sim_data = sim_data,
  reg_equation = reg_equation,
  target_cor = target_cor,
  target_reg = target_reg,
  target_se = target_se,
  weight = weight.lm,
  tol = 1e-8,
  max_iter = 1e5,
  max_starts = 1,
  init_temp = 1,
  cooling_rate = NULL
)
```

I then plotted these RMSE distributions side-by-side to compare within versus between run variation.
```{r plot rmse lm, fig.width=10, fig.height=5, fig.alt="Plot RMSE for LM."}
plot_rmse(result.parallel.lm)
```
